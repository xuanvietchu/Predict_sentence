{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import MBartTokenizerFast, MBartForConditionalGeneration, AutoTokenizer\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers.optimization import AdamW\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH = 200, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"../models/bartpho_paws_qqp/epoch_5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\")\n",
    "\n",
    "# model = MT5ForConditionalGeneration.from_pretrained('./models/mt5-base-newer/epoch_9')\n",
    "# tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "def main(sent, num_outputs: int = 10, max_length: int = 200):\n",
    "    generated = model.generate(\n",
    "        tokenizer.encode(sent, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt').to(device),\n",
    "        num_beams=10, num_return_sequences=num_outputs, max_length=max_length\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for generated_sentence in generated:\n",
    "        # print(generated_sentence)\n",
    "        # print(len(generated_sentence))\n",
    "        out = tokenizer.decode(\n",
    "                generated_sentence,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        result.append(out)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_THRESHOLD = 2\n",
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    final_distance = distances[-1]\n",
    "    score = final_distance / max(len(s1), len(s2))\n",
    "    return {\n",
    "        \"raw_distance\": final_distance,\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "def filter_answer(raw_sentence, paraphrase_sentences):\n",
    "    output = []\n",
    "    for sen in paraphrase_sentences:\n",
    "        distance_score = levenshteinDistance(raw_sentence, sen)\n",
    "        raw_distance = distance_score[\"raw_distance\"]\n",
    "        score = distance_score[\"score\"]\n",
    "        if raw_distance >= DISTANCE_THRESHOLD:\n",
    "            output.append(sen)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Người ra đi đầu không ngoảnh lại, sau lưng thềm nắng lá rơi đầy.\"\n",
    "paraphrases = main(text)\n",
    "\n",
    "output = filter_answer(text, paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [text] + output\n",
    "\n",
    "compare = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "embeddings = compare.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Người ra đi đầu không ngoảnh lại, sau lưng thềm nắng lá rơi đầy. tensor(1.0000)\n",
      "Đi đầu không ngoảnh lại, sau lưng thềm nắng lá rơi đầy. tensor(0.9829)\n",
      "Đi đầu không ngoảnh lại, lưng thềm nắng lá rơi đầy. tensor(0.9689)\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(embeddings):\n",
    "    print(sentences[i], util.cos_sim(embeddings[0], sentence)[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase",
   "language": "python",
   "name": "paraphrase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
